{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b57a3ee0-117f-4127-8e36-a540f7a535dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#python -m nltk.downloader all\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52070d-3797-4031-ad53-2a605ce838f2",
   "metadata": {},
   "source": [
    "**Cargar los datos y separarlos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd802f89-0bc1-4aea-8358-1749eda83c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textos</th>\n",
       "      <th>ODS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Aprendizaje\" y \"educación\" se consideran sinó...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No dejar clara la naturaleza de estos riesgos ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Como resultado, un mayor y mejorado acceso al ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Con el Congreso firmemente en control de la ju...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luego, dos secciones finales analizan las impl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              textos  ODS\n",
       "0  \"Aprendizaje\" y \"educación\" se consideran sinó...    4\n",
       "1  No dejar clara la naturaleza de estos riesgos ...    6\n",
       "2  Como resultado, un mayor y mejorado acceso al ...   13\n",
       "3  Con el Congreso firmemente en control de la ju...   16\n",
       "4  Luego, dos secciones finales analizan las impl...    5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_excel('Train_textosODS.xlsx')\n",
    "file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29aff16b-0c2d-4f21-8f85-82af45b20d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODS = file['ODS']\n",
    "textos = file['textos']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077dab0-edd1-4ffb-a546-81e20c830e6b",
   "metadata": {},
   "source": [
    "**Clase para procesar/preparar el texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a291bc55-dd7f-47ff-b4e3-c720e7e9badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    A text processing class for text data. This transformer handles tokenization, \n",
    "    stop word removal, and stemming (optional), and transforms text data into \n",
    "    a bag-of-words representation.\n",
    "    '''\n",
    "    def __init__(self, token_cue = r'\\w+', language = 'spanish', stemming_switch = True):\n",
    "        self.language = language\n",
    "        self.token_cue = token_cue\n",
    "        self.stop_words_list = stopwords.words(self.language)\n",
    "        self.tokenizer = RegexpTokenizer(self.token_cue)\n",
    "        self.stemming_switch = stemming_switch\n",
    "        self.stemmer = SnowballStemmer(self.language)\n",
    "        self.vectorizer = TfidfVectorizer()       \n",
    "        \n",
    "    def fit(self, text, y = None):\n",
    "        # Implementation to comply with pipeline API requirements\n",
    "        return self\n",
    "\n",
    "    def tokenize(self,text): #TODO\n",
    "        return text.apply(lambda x: self.tokenizer.tokenize(x))\n",
    "        \n",
    "    def remove_stop_words(self,text): #TODO\n",
    "        return text.apply(lambda x: [token for token in x if token not in self.stop_words_list])\n",
    "        \t\n",
    "    def stemming(self,text):#TODO\n",
    "        #return text.apply(lambda x: [self.stemmer.stem(token) for token in x])\n",
    "        return text.apply(lambda x: [str(self.stemmer.stem(token)) for token in x])\n",
    "\n",
    "    def BOW_transform(self,stems):        \n",
    "        return self.vectorizer.fit_transform(stems.apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    def transform(self,text, y = None):\n",
    "        if self.stemming_switch:\n",
    "            return self.BOW_transform(self.stemming(self.remove_stop_words(self.tokenize(text)))) # With Stemming\n",
    "        else: \n",
    "            return self.BOW_transform(self.remove_stop_words(self.tokenize(text))) # Without Stemming    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdfbc1-993f-4f23-a454-dcb8dc916e18",
   "metadata": {},
   "source": [
    "**Definir el Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "628e76f5-3b7f-4081-8b29-517a897c0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=100)\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "pipeline = Pipeline([\n",
    "    ('text_processor', TextProcessor()),\n",
    "    ('tsvd', TruncatedSVD(n_components=100)),\n",
    "    ('logreg', LogisticRegression(max_iter=500))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a26426-ce9e-4ed8-bdb6-109aeae7ec7d",
   "metadata": {},
   "source": [
    "**Entrenar el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9c7f513-5b47-4727-9e77-d43caa7bef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;text_processor&#x27;, TextProcessor()),\n",
       "                (&#x27;tsvd&#x27;, TruncatedSVD(n_components=100)),\n",
       "                (&#x27;logreg&#x27;, LogisticRegression(max_iter=500))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;text_processor&#x27;, TextProcessor()),\n",
       "                (&#x27;tsvd&#x27;, TruncatedSVD(n_components=100)),\n",
       "                (&#x27;logreg&#x27;, LogisticRegression(max_iter=500))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TextProcessor</label><div class=\"sk-toggleable__content\"><pre>TextProcessor()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=100)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('text_processor', TextProcessor()),\n",
       "                ('tsvd', TruncatedSVD(n_components=100)),\n",
       "                ('logreg', LogisticRegression(max_iter=500))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar los datos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(textos, ODS, test_size = 0.2, random_state=0)\n",
    "# Fit el pipeline\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4fbcf-7ccd-4f70-bd0f-27ddea7d1138",
   "metadata": {},
   "source": [
    "**Resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1364712f-0cdd-4806-b6f8-4b26cb47ad2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 8409 features, but TruncatedSVD is expecting 16500 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# predecir los datos de prueba\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# sacar metricas\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:507\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    505\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 507\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:292\u001b[0m, in \u001b[0;36mTruncatedSVD.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    Reduced version of X. This will always be a dense array.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 292\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8409 features, but TruncatedSVD is expecting 16500 features as input."
     ]
    }
   ],
   "source": [
    "# predecir los datos de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# sacar metricas\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba02a0-babb-4011-a85f-b3d6ee8e578d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3a3e7-e8d5-4908-8302-99e75d1e7791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059f2dc-094f-47b1-8915-3e0fa4af1e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d2d4752-6fa7-44e8-981c-585e6f19eef0",
   "metadata": {},
   "source": [
    "**Celdas de Prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656ca5f-7072-4163-961d-def1844bac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = procesador.tokenize(X_train[0:20])\n",
    "# no_stop = procesador.remove_stop_words(tokens)\n",
    "# stems = procesador.stemming(no_stop)\n",
    "# BOW = procesador.BOW_transform(stems)\n",
    "# BOW = procesador.BOW_transform(stems)\n",
    "# BOW[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2dd09-6599-47cd-9010-aac35100abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=100)\n",
    "# BOW_reduced = svd.fit_transform(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddab2b3-ead1-4666-be50-6b5cc130d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = LogisticRegression(max_iter=500)\n",
    "# reg.fit(BOW_reduced,y_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd178b1-d237-4cb7-a6f3-b753b4c4d519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079ee85-c71e-4aad-aab2-69cfcdbc131c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
